{% extends "base.twig" %}
{% block title %}About{% endblock %}
{% block content %}
    <div class="row">
        <div class="col-md-12">
            <h1>Setup guide</h1>
        </div>
    </div>
	
	<div class="row">
		<div class="col-md-8">
			<p>
				The entire implementation is split into two parts. The first part is a pure Python based implementation that covers all the phases starting from data extraction to analysis. The second part focuses on visualization(i.e this webpage), the entire implementation is built around the database file produced in first step. All the necessary dependencies are specified in the <i>requirements.txt</i> file, located in the project root folder. A simple Makefile can be used to perform individual process steps, clean current results or install the project library dependecies. The following make commands are supported:
			</p>
			
			<ul>
				<li>
					<b>init</b> - uses pip to install dependencies specified in requirements.txt file.
				</li>
				<li>
					<b>clean-pyc</b> - deletes all compiled python bytecode files from project folder.
				</li>
				<li>
					<b>clean-analysis</b> - deletes previous results, e.g gathered faculty member names, analysis results. Rebuilds the database or creates it if missing all-together. Note that in order to fully delete all previous results(e.g raw files and the corresponding database entries), an additional parameter <i>fc</i> has to be provided with the value of <i>1</i>.
				</li>
				</li>
					<b>clean-stale</b> - deletes all files and database entries not related to target semesters. The target semesters can be specified via <i>SEMESTERS</i> parameter.
				</li>
				<li>
				<b>scrape-courses</b> - performs data scraping over courses web page. The command requires the specification of a <i>SEMESTERS</i> parameter. The value is expected to be a comma delimited list of semesters to scrape. A single semester can be given as a concatenation of year and the first letter of the semester by season(i.e F for Fall and S for Spring). For example the following command can be used to scrape data based on 2015/Spring and 2015/Fall semesters: <i>make scrape-courses SEMESTERS=2015S,2015F</i>.
				</li>
				<li>
					<b>scrape-moodle</b> - scrape data from Moodle, this is a complementary command that should be run only after scraping Courses web page. As an additional condition, the target semesters for Courses web crawling had to contain the currently ongoing semester.
				</li>
				<li>
					<b>scrape-teachers</b> - scrape the names of the faculty members and place them in file in JSON format. The list of names are used as a filter during data cleaning phase.
				</li>
				</li>
					<b>extract-sis</b> - Load data from a <i>csv</i> file that was extracted from Study Information System database. The command is complementary and requires the specification of <i>SEMESTERS</i> parameter.
				</li>
				<li>
					<b>extract</b> - extract textual content from downloaded course materials.
				</li>
				<li>				
					<b>tokenize</b> - Perform data cleaning, this includes tokenization, acronym extraction and finding frequently co-occurring words.
				</li>
				<li>
					<b>analyse</b> - Perform topic modelling at a course and course material level, this includes resolving topic titles.				
				</li>
			</ul>
			
			<p>
				A shell script <i>analyse_courses.sh</i> can be used to perform all the steps in a sequential order, requiring only the specification of initial parameters.
			</p>
			
			<p>
				<div class="code">
			    <i>usage: analyse_courses [[[-s semesters ] [-sis] [-t] [-tdir] [-m] [-fc]] | [-h]]</i>
				<div class="code">
					<b>-s, --semesters semesters</b> : comma separated list of semesters to be scraped from courses web page. E.g 2015F,2016S<br>
					<b>-m, --moodle</b> : scrape data from moodle<br>
					<b>-t, --teachers</b> : scrape teacher names to exclude them from analysis<br>
					<b>-sis, --studyinfosystem</b> : extract study information system data from .csv file<br>
					<b>-tdir, --targetdirectory</b> : directory where the resulting DB file will be copied<br>
					<b>-fc, --fullclean</b> : remove all existing data including corresponding DB entries. Re-download everything instead of just updating the missing parts<br>
					<b>-h, --help</b> : display help<br>
				</div>
				</div>
			</p>
			
			<p>
				For example the following command can be used to run the process, targeting 2015/2016 and 2016/2017 academic years:
			</p>
			
			<p>
				<div class="text-center">
					<i>bash ./analyse_courses.sh -s 2015F,2016S,2016F,2017S -m -sis -t -fc</i>
				</div>
			</p>				
			
			<p>
			The visualization part, however, requires the set up of Apache web server with a fitting PHP version. The visualization part uses PHP for backend integration, HTML, Javascript(for various charts and graphs) and Bootstrap for frontend. The necessary PHP dependencies(e.g Silex framework) are specified in composer.json file and can be acquired via Composer dependency manager.
			</p>
			
			<p>
				Concerning the integration between part one, where we perform the analysis, and part two, where we visualize the results, the only necessary action is to copy the database produced in part one to the <i>db</i> folder in the project of part two. This small step can be automized by providing the <i>tdir</i> parameter, when running <i>analyse_courses.sh</i> script. All-in-all, if the Apache server is already running, then the only thing needed to update the results is the successful execution of <i>analyise_courses.sh</i> script.
			<p>
				Additionally, it is possible to add new Courses or Moodle web pages as they become available for different faculties by adding the web scraping start points in the configuration file, i.e <i>utils/config.cfg</i>. A similar URL pattern(e.g language is explicitly specified in URL) should be used, when adding new starting points.
			</p>
		</div>
		
        <div class="col-md-4">
            <p>
                Following tools were used to build the framework:
                <ul>
                    <li><a href="http://scrapy.org/">Scrapy</a> for crawling and downloading all the data from the Courses websites.</li>
                    <li><a href="http://euske.github.io/pdfminer/index.html">PDFMiner</a> for extracting all the textual data from the PDF files.</li>
                    <li><a href="https://python-pptx.readthedocs.org/en/latest/">python-pptx</a> for extracting all the textual data from the PPTX files.</li>
                    <li><a href="https://python-docx.readthedocs.io/en/latest/">python-docx</a> for extracting all the textual data from the DOCX files.</li>
					<li><a href="https://github.com/phfaist/pylatexenc">pylatexenc</a> for extracting all the textual data from the TEX files.</li>
                    <li><a href="http://www.crummy.com/software/BeautifulSoup/">Beautiful Soup</a> for extracting all the textual data from the HTML content pages.</li>
                    <li><a href="http://www.nltk.org/">NLTK</a> for tokenizing text and lemmatizing the extracted English words into main form to reduce duplicate words.</li>
                    <li><a href="http://pythonhosted.org/lda/">LDA</a> for topic modeling.</li>
                    <li><a href="http://timdream.org/wordcloud2.js/">WordCloud</a> for creating word clouds of top-most words.</li>
                    {# <li><a href="http://d3js.org/">D3js</a> for pie-charts.</li> #}
                    <li><a href="http://www.highcharts.com/">Highcharts</a> for creating a heatmap of course topics.</li>
                    <li><a href="https://www.sqlite.org/">SQLite</a> and <a href="https://peewee.readthedocs.org/en/latest/">peewee</a> for storing data.</li>
                    <li><a href="http://silex.sensiolabs.org/">Silex</a> and <a href="http://getbootstrap.com/">Bootstrap</a> for creating this website.</li>
					<li><a href="https://pypi.python.org/pypi/langdetect/1.0.5">langdetect</a> for detecting the language of a text.</li>
					<li><a href="https://github.com/estnltk/estnltk">estnltk</a> for lemmatizing Estonian words.</li>
				</ul>
            </p>
			
			<p>
				The projects are available in following GitHub repositories:
				<ul>
					<li>
						<a href="https://github.com/ragnarvent/courses_analysis">Data gathering, cleaning and analysis</a>
					</li>
					<li>
						<a href="https://github.com/ragnarvent/courses_analysis_web">Project website</a>
					</li>
				</ul>
			</p>
        </div>
	</div>
{% endblock %}
